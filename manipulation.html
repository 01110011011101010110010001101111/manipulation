<!DOCTYPE html>

<html>

  <head>
    <title>Intelligent Robot Manipulation</title>
    <meta name="Intelligent Robot Manipulation"
          content="text/html; charset=utf-8;" />

    <script type="text/javascript" src="htmlbook/book.js"></script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        skipStartupTypeset:true,
        extensions: ["tex2jax.js"],
        jax: ["input/TeX", "output/HTML-CSS"],
        TeX: { equationNumbers: {autoNumber: "AMS"}, noErrors: { disabled: true } },
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"] ],
          displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
          processEscapes: true
        },
        "HTML-CSS": { availableFonts: ["TeX"] }
      });
    </script>
    <script type="text/javascript"
      src="htmlbook/MathJax/MathJax.js?config=TeX-AMS_HTML">
    </script>

    <link rel="stylesheet" href="highlight/styles/default.css">
    <script src="highlight/highlight.pack.js"></script> <!-- http://highlightjs.readthedocs.io/en/latest/css-classes-reference.html#language-names-and-aliases -->
    <script>hljs.initHighlightingOnLoad();</script>

    <link rel="stylesheet" type="text/css" href="htmlbook/book.css">
  </head>


<body data-type="book" class="book" id="htmlbook" onload="revealChapters();">
<section data-type="titlepage" class="titlepage">
  <header>
    <h1><a href="manipulation.html"
    style="text-decoration:none;">Intelligent Robot Manipulation
    </a></h1>
    <p data-type="subtitle">A Systems Theory Perspective on Perception, Planning, and
      Control</p>
  	<p style="font-size: 18px;"><a href="http://people.csail.mit.edu/russt/">Russ Tedrake</a></p>
    <p style="font-size: 14px; text-align: right;">
      &copy; Russ Tedrake, 2019<br/>
      <a href="tocite.html">How to cite these notes</a><br/>
    </p>
  </header>
</section>
<div style="display:none" id="mathjax_setup"> <!-- definitions for mathjax -->
  \[
  \newcommand{\pd}[2]{\frac{\partial #1}{\partial #2}}
  \newcommand{\bc}{{\bf c}}
  \newcommand{\bg}{{\bf g}}
  \newcommand{\bh}{{\bf h}}
  \newcommand{\bq}{{\bf q}}
  \newcommand{\bv}{{\bf v}}
  \newcommand{\bx}{{\bf x}}
  \newcommand{\by}{{\bf y}}
  \newcommand{\bu}{{\bf u}}
  \newcommand{\bw}{{\bf w}}
  \newcommand{\bz}{{\bf z}}
  \newcommand{\bA}{{\bf A}}
  \newcommand{\bB}{{\bf B}}
  \newcommand{\bC}{{\bf C}}
  \newcommand{\bH}{{\bf H}}
  \newcommand{\bI}{{\bf I}}
  \newcommand{\bJ}{{\bf J}}
  \newcommand{\bK}{{\bf K}}
  \newcommand{\bM}{{\bf M}}
  \newcommand{\bQ}{{\bf Q}}
  \newcommand{\bR}{{\bf R}}
  \newcommand{\bT}{{\bf T}}
  \newcommand{\balpha}{{\bf \alpha}}
  \newcommand{\bbeta}{{\bf \beta}}
  \newcommand{\blambda}{{\bf \lambda}}
  \newcommand{\btau}{{\bf \tau}}
  \newcommand{\bphi}{{\bf \phi}}
  \newcommand{\bPhi}{{\bf \Phi}}
  \newcommand{\bpsi}{{\bf \psi}}
  \newcommand{\bPsi}{{\bf \Psi}}
  \newcommand{\avg}[1]{E\left[ #1 \right]}
  \newcommand{\subjto}{\textrm{subject to}}
  \newcommand{\minimize}{\operatorname{\textrm{minimize}}}
  \newcommand{\maximize}{\operatorname{\textrm{maximize}}}
  \DeclareMathOperator*{\find}{find}
  \newcommand{\argmax}{\operatorname{\textrm{argmax}}}
  \newcommand{\argmin}{\operatorname{\textrm{argmin}}}
  \newcommand{\sgn}{\operatorname{\textrm{sgn}}}
  \newcommand{\trace}{\operatorname{\textrm{tr}}}
  \newcommand{\sos}{\text{ is SOS}}
  \]
</div>

<div id="debug_output"></div>

<p><b>Note:</b> These are working notes used for <a
href="http://manipulation.csail.mit.edu/">a course being taught at
MIT</a>. They will be updated throughout the Fall 2019 semester. </p>

<section id="table_of_contents"></section>


<section id="preface"><h1>Preface</h1>

  <p>I've always loved robots, but it's only relatively recently that I've
  turned my attention to robotic manipulation.  I particularly like the
  challenge of building robots that can master physics to achieve
  human/animal-like dexterity and agility.  It was
  <a
  href="http://underactuated.mit.edu/underactuated.html?chapter=intro">passive
  dynamic walkers</a> and the beautiful analysis that accompanies them that
  first helped cement this centrality of dynamics in my view of the world and my
  approach to robotics.  From there I became fascinated with (experimental)
  fluid dynamics, and the idea that birds with articulated wings actually
  "manipulate" the air to achieve incredible efficiency and agility. Humanoid
  robots and fast-flying aerial vehicles in clutter forced me to start thinking
  more deeply about the role of perception in dynamics and control.  Now I
  believe that this interplay between perception and dynamics is truly
  fundamental, and I am passionate about the observation that relatively
  "simple" problems in manipulation (how do I button up my dress shirt?) expose
  the problem beautifully.</p>

  <p>My approach to programming robots has always been very
  computational/algorithmic.  I started out using tools primarily from machine
  learning (especially reinforcement learning) to develop the control systems
  for simple walking machines; but as the robots and tasks got more complex I
  turned to more sophisticated tools from model-based planning and
  optimization-based control.  In my view, no other discipline has thought so
  deeply about dynamics as has control theory, and the algorithmic efficiency
  and guaranteed performance/robustness that can be obtained by the best
  model-based control algorithms far surpasses what we can do today with
  learning control.  Unfortunately, the mathematical maturity of
  controls-related research has also led the field to be relatively conservative
  in their assumptions and problem formulations; the requirements for robotic
  manipulation break these assumptions.  For example, robust control typically
  assumes that dynamics that are (nearly) smooth and uncertainty that can be
  represented by e.g. ellipsoidal or polytopic sets; but in robotic manipulation
  we must deal with the non-smooth mechanics of contact and uncertainty the
  comes from varied lighting conditions, and different numbers of objects with
  unknown geometry and dynamics.  In practice, no state-of-the-art robotic
  manipulation system to date (that I know of) uses rigorous control theory to
  design even the low-level feedback that determines when a robot makes and
  breaks contact with the objects it is manipulating.  An explicit goal of these
  notes is to try to change that.</p>

  <p>In the past few years, deep learning has had an unquestionable impact on robotic perception, unblocking some of the most daunting challenges in performing manipulation outside of a laboratory or factory environment.  We will discuss relevant tools from deep learning for object recognition, segmentation, pose/keypoint estimation, and even shape completion.  Now relatively old approaches to learning control are also enjoying an incredible surge in popularity, fueled in part by massive computing power and increasingly available robot hardware and simulators.  Unlike learning for perception, learning control algorithms are still far from a technology, with some of the most impressive looking results still being hard to understand and to reproduce.  But the recent work in this area has unquestionably highlighted the pitfalls of the conservatism taken by the controls community. Learning researchers are boldly formulating much more aggressive and exciting problems for robotic manipulation than we have seen before -- in many cases we are realizing that some manipulation tasks are actually quite easy, but in other cases we are finding problems that are still fundamentally hard.
  </p>

  <p>Finally, it feels that the time is ripe for robotic manipulation to have a real and dramatic impact in the world, in fields from logistics to home robots.  Over the last few years, we've seen UAVs/drones transition from academic curiosities into consumer products; autonomous cars have been transitioning from academic research to industry now, at least in terms of dollars invested.  Manipulation feels like the next big thing that will transition from robotic research to practice. It's still a bit risky for a venture capitalist to invest in, but nobody doubts the size of the market once we have the technology.  How lucky are we to potentially be able to play a role in that transition?</p>

  <p>So this is where the notes begin... we are at an incredible crossroads between learning and control and robotics with an opportunity to have immediate impact in industrial and consumer applications and potentially even to forge entirely new eras for systems theory and controls.  I'm just trying to hold on and to enjoy the ride.</p>

<!---
    <p>Data clearly has an immensely important role to play... (discussion slides from L4DC).  A pure learning approach is like sending our newborns out to play with the wolves.  Models and simulation just need to provide a sufficiently rich sand-box.  And it definitely seems short-sighted to bet against simulation.</p>
-->

</section> <!-- end preface -->

<section data-type="chapter" class="chapter" id="intro">
  <h1>Introduction</h1>

  <p>It's worth taking time to appreciate just how amazingly well humans are
  able to perform tasks with our hands.  Tasks that often feel mundane to us --
  loading the dishwasher, chopping vegetables, folding laundry -- remain as
  incredibly challenging tasks for robots and are at the very forefront of
  robotics research.</p>
  
  <todo>Add a diagram / photo of a the dish task here?</todo>

  <p>Consider the problem of picking up a single plate from a stack of plates in
  the sink and placing it into the dishwasher.  Clearly you first have to
  perceive that there is a plate in the sink and that this it is accessible.
  Getting your hand to the plate likely requires navigating your hand around the
  geometry of the sink and other dishes, and the act of actually picking it up
  might require a fairly subtle maneuver where you have to tip up the plate, sliding it along your fingers and along the sink/dishes in order to get a reasonable grasp on it.  Presumably as you lift it out of the sink, you'd like to mostly avoid collisions between the plate and the sink, which suggests a reasonable understanding of the size/extent of the plate (though note that robots tend to avoid collisions much more than humans do).  Even placing the plate into the dishwasher is pretty subtle; you might think that you would align the plate with the slats and then slide it in, but I think humans are more clever than that.  A seemingly better strategy is to loosen your grip on the plate, come in at an angle and intentionally contact one side of the slat... letting the plate effectively rotate itself into position as you set it down.  But the justification for this strategy is subtle -- it is a way to achieve the kinematically accurate task without requiring much kinematic accuracy on the position/orientation of the plate.</p>

  <todo>insert video of TRI dish pickup here</todo>

  <p>Perhaps one of the reasons that these problems remain so hard is that they
  require strong capabilities in numerous technical areas that have
  traditionally been somewhat disparate -- it's hard to be an expert in all of
  them.  More so than robotic mapping and navigation, or legged locomotion, or
  other great areas in robotics, the most interesting problems in manipulation
  require significant interactions between perception, planning, and control.
  This includes both geometric perception to understand the local geometry of
  the objects and environment, but also semantic perception to understand what
  opportunities for manipulation are avaiable in the scene. Planning typically
  includes both reasoning about the kinematic constraints of the task (how to I
  command my rigid seven degree-of-freedom arm to reach into the drawer?) as
  well as higher-level task planning (to get milk into my cup, I need to open
  the fridge, then grab the bottle, then unscrew the lid, then pour.. and
  perhaps even put it all back) that require understanding these kinematic
  constraints as well as the higher level task semantics.  And the lowest level,
  our hands are making and breaking contact with the world either directly or
  through tools, exerting forces, rapidly and easily transitioning between
  sticking and sliding frictional regimes -- these are incredible rich and
  difficult problems from the perspective of dynamics and control.</p>

  <!-- two core research problems (that I like to focus on): 1) there are many tasks that we don't know how to program a robot to do robustly even in a single instance in lab (reach into your pocket and pull out the keys); 2) achieving robustness of a complete manipulation stack in the open-world. -->

  <section data-type="sect1"><h1>Manipulation is more than Pick-and-Place</h1></section>

    <p>There are a large number of applications for manipulation.  Picking up an
    object from one bin and placing it into another bin -- one version of the
    famous "pick and place" problem -- is a great application for robotic 
    manipulation.  And it is an application for which we already have pretty
    capable solutions, especially if the location/orientation of the placement
    need not be very accurate.  The recent advances in computer vision for
    object recognition make it very useful for a wide variety of industry
    applications: a camera can look into a bin and find an object of interest,
    then use a relatively simple strategy to do the pick.  This can be done with
    conventional robot hands or more special-purpose hands that are equipped
    with suction.  It can often be done without having a very accurate understanding of the shape, pose, mass, nor friction of the object(s) to be picked.</p>

    <p>The goal for these notes, however, is to examine the much broader view of
    manipulation that we is captured by the pick and place problem.  Even our thought experiment of loading the dishwasher -- arguably a more advanced type of pick and place -- requires much more from the perception, planning, and control systems.  But the diversity of tasks that humans (and hopefully soon robots) can do with their hands is truly remarkable.</p>

    <p>One field that has thought about the Motivating example: <a href="http://www.shap.ecs.soton.ac.uk/">South Hampton Assessment Procedure (SHAP)</a></p>

  <section data-type="sect1"><h1>Open-World Manipulation</h1>
  
    <p>Perhaps because humans are so good at manipulation, our expectations in terms of performance and robustness for these tasks is extremely high.  It's not enough to be able to load one set of place in a laboratory environment into a dishwasher reliably.  We'd like to be able to manipulate basically any plate that someone might put into the sink.  And we'd like the system to be able to work in any kitchen, despite various geometric configurations, lighting conditions, etc.  The challenge of achieving and verifying robustness of a complex manipulation stack with physics, perception, planning, and control in the loop is already daunting.  But how do we provide test coverage for every kitchen in the world?</p>

    <p>The idea that the world has infinite variability (there will never be a point at which you have seen every possible kitchen) is often referred to as the "open-world" or "open-domain" problem -- a term popularized first in the context of <a href="https://en.wikipedia.org/wiki/Open_world">video games</a>.</p>

    <p>Might it actually make the problem easier?  (Diversity changes the optimization landscape).</p>
  </section>

  <section data-type="sect1"><h1>Simulation</h1>
  
  </section>

  <section data-type="sect1">
    <h1>A Systems Theoretic Approach</h1>
  
    <blockquote>It's precisely <i>because</i> the problem is so complex that we take a rigorous approach.</blockquote>
  
    <p>We could draw this sort of diagram for many modern manipulation systems — they are often composed of numerous modules that communicate with eg IPC.  But i am going to demand a bit more of our models.  Explixitly declare their state, their parameters (just the ones that we would like to tune/analyze automatically), and any randomness.  This opens the door for more mature approaches to testing and verification, and ultimately for synthesis.  But it is surprisingly rare in robotics today. </p>
  
    <p>Probably don’t have to convince anyone about the value of the modularity.  Might have to convince about the value of declaring state, etc.  it will be a rexurring theme in this book.</p>
    
    <p>Using a drake system in your favorite ipc stack is easy.  We also make provisions for going the other way — taking your black-box component and using it in the systems framework.  But this limits which algorithms we can use on it.</p>
  
    <p>Idea: perception spoof.  Same input/output, mich simpler dynamics.  Ground truth + noise + dropouts.</p>
  
    <p>Runtime monitoring, system id, etc</p>
  
    <p>One thing that we’ll leave out is humans.  It’s not that i don’t like humans, but we have enough to do here without trying to model them.  Let’s call it future work.  The one exception is when we talk about control of the arm — a few simple heuristics at this level can make the robots much safer if operating around humans.  The robot won’t understand them, but we want to make sure we still don’t hurt them.  Or other robots/agents in the scene.</p>
  
  </section> <!-- end chapter (systems)-->

  <section data-type="sect1"><h1>Components of a Modern Manipulation System</h1>

    <p>The remaining chapters of these notes are organized...</p>
    <p>The central role of modeling and simulation.</p>
    <p>Robot Hardware.  Mathematical models.</p>

  </section>

</section> <!-- end chapter (intro)-->


  

<section data-type="chapter" class="chapter" id="hardware">
  <h1>Manipulation Hardware</h1>

  <todo>insert the manipulation station block diagram image here.</todo>
	
  <section data-type="sect1"><h1>Arms, Hands, Sensors, ...</h1></section>

  <section data-type="sect1"><h1>Low-cost arms</h1></section>

  <section data-type="sect1"><h1>Dexterous Hands</h1></section>

  <section data-type="sect1"><h1>Soft Hands/Skins</h1></section>

  <section data-type="sect1"><h1>Tactile Sensors</h1></section>

</section>

<section data-type="chapter" class="chapter" id="kinematics">
  <h1>Kinematics and Dynamics</h1>

</section>

<section data-type="chapter" class="chapter" id="pose">
  <h1>Geometric Perception I: Pose Estimation and Tracking</h1>

  <section data-type"sect1"><h1>Depth Sensors</h1>
  
    <p>Primarily RGB-D (ToF vs projected texture stereo vs ...) cameras and Lidar</p>

    <p>The cameras we are using in this course are Intel RealSense D415....</p>
  
  </section> 

  <section data-type="sect1"><h1>Working with Point Clouds</h1>
  
    <p>We'll use <a href="http://www.open3d.org/">Open3D</a> (note that many older projects used <a href="http://pointclouds.org/">PCL</a>, which is now defunct).</p>

    <section data-type="sect2"><h1>Plane Fitting and Normal
    Estimation</h1></section>

  </section>

  <section data-type="sect1"><h1>Iterative Closest Point (ICP)</h1>
  
    <todo>Add a simple example and a homework problem/exercise.</todo>
  
  </section>

  <section data-type="sect1"><h1>Coherent Point Drift (CPD)</h1>

    <section data-type="sect2"><h1>FilterReg</h1></section>

  </section>

  <section data-type="sect1"><h1>Dense Articulated Real-Time Tracking
  (DART)</h1></section>

</section>

<section data-type="chapter" class="chapter" id="planning">
  <h1>Basic Planning and Control</h1>

  <section data-type="sect1"><h1>Task-space control</h1></section>

  <section
          data-type="sect1"><h1>Kinematic trajectory optimization</h1></section>

  <section data-type="sect1"><h1>Key-point formulations</h1></section>

</section>

<section data-type="chapter" class="chapter" id="grasping">
  <h1>Grasping</h1>

  <section data-type="sect1"><h1>Anti-podal grasps</h1></section>

  <section data-type="sect1"><h1>Grasp optimization</h1></section>

  <section data-type="sect1"><h1>Learning grasps</h1></section>

</section>

<section data-type="chapter" class="chapter" id="task">
    <h1>Programming the Task Level Execution</h1>

    <section data-type="sect1"><h1>Behavior Trees</h1></section>


</section>

<section data-type="chapter" class="chapter" id="fusion">
  <h1>Geometric Perception II: Dense Reconstructions</h1>

  <section data-type="sect1"><h1>Kinect Fusion</h1></section>

  <section data-type="sect1"><h1>Elastic Fusion</h1></section>

  <section data-type="sect1"><h1>SurfelWarp</h1></section>

</section>

<section data-type="chapter" class="chapter" id="planning">
  <h1>Planning in Cluttered Environments</h1>

</section>

<section data-type="chapter" class="chapter" id="perception">
  <h1>Deep Learning for Perception</h1>

  <section data-type="sect1"><h1>Getting training data</h1>

    <section data-type="sect2"><h1>LabelFusion</h1></section>

    <section data-type="sect2"><h1>Synthetic Datasets</h1></section>
  </section>

  <section data-type="sect1"><h1>Segmentation</h1></section>

  <section data-type="sect1"><h1>Object recognition</h1></section>

  <section data-type="sect1"><h1>Pose estimation</h1></section>

  <section data-type="sect1"><h1>Keypoint Detection</h1></section>

  <section data-type="sect1"><h1>Dense Descriptors</h1></section>

</section>

<section data-type="chapter" class="chapter" id="control">
  <h1>Feedback Control</h1>

</section>

<section data-type="chapter" class="chapter" id="planning">
  <h1>Non-Prehensile Manipulation</h1>

  <section data-type="sect1"><h1>Planning through Contact</h1></section>
  
</section>
  
<section data-type="chapter" class="chapter" id="belief">
  <h1>Planning under Uncertainty</h1>

</section>


<section data-type="chapter" class="chapter" id="tamp">
  <h1>Task and Motion Planning</h1>

</section>

<section data-type="chapter" class="chapter" id="rl">
  <h1>Reinforcement Learning</h1>

</section>

<section data-type="chapter" class="chapter" id="intuitive">
    <h1>Intuitive Physics</h1>

</section>

<section data-type="chapter" class="chapter" id="imitation">
  <h1>Teleoperation and Imitation Learning</h1>

  <!-- Not exactly sure where I want to put this, but it seems useful to have.  My current view is that this is an extremely useful set of tools for exploring the space of solutions, but probably not a final solution...  -->
  <p>Interfaces.  Inputs (keyboard + mouse, HTC Vive, data gloves...).  Feedback (VR, haptic feedback).</p>

</section>

<todo>Maybe a chapter on tactile feedback (estimation, tracking, slip detection, etc)?</todo>

<section data-type="chapter" class="chapter" id="v_and_v">
  <h1>Verification and Validation</h1>

  <p>P(world)</p>

</section>

<appendix class="part" title="Appendix">



</appendix>


<div id="footer">
<hr>
<table style="width:100%;">
  <tr><td><em>Intelligent Robot Manipulation</em></td><td align="right">&copy;
    Russ
    Tedrake, 2019</td></tr>
</table>
</div>


</body>
</html>
