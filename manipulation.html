<!DOCTYPE html>

<html>

  <head>
    <title>Intelligent Robot Manipulation</title>
    <meta name="Intelligent Robot Manipulation"
          content="text/html; charset=utf-8;" />

    <script type="text/javascript" src="htmlbook/book.js"></script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        skipStartupTypeset:true,
        extensions: ["tex2jax.js"],
        jax: ["input/TeX", "output/HTML-CSS"],
        TeX: { equationNumbers: {autoNumber: "AMS"}, noErrors: { disabled: true } },
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"] ],
          displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
          processEscapes: true
        },
        "HTML-CSS": { availableFonts: ["TeX"] }
      });
    </script>
    <script type="text/javascript"
      src="htmlbook/MathJax/MathJax.js?config=TeX-AMS_HTML">
    </script>

    <link rel="stylesheet" href="highlight/styles/default.css">
    <script src="highlight/highlight.pack.js"></script> <!-- http://highlightjs.readthedocs.io/en/latest/css-classes-reference.html#language-names-and-aliases -->
    <script>hljs.initHighlightingOnLoad();</script>

    <link rel="stylesheet" type="text/css" href="htmlbook/book.css">
  </head>


<body data-type="book" class="book" id="htmlbook" onload="revealChapters();">
<section data-type="titlepage" class="titlepage">
  <header>
    <h1><a href="manipulation.html"
    style="text-decoration:none;">Intelligent Robot Manipulation
    </a></h1>
    <p data-type="subtitle">A Systems Theory Perspective on Perception, Planning, and
      Control</p>
  	<p style="font-size: 18px;"><a href="http://people.csail.mit.edu/russt/">Russ Tedrake</a></p>
    <p style="font-size: 14px; text-align: right;">
      &copy; Russ Tedrake, 2019<br/>
      <a href="tocite.html">How to cite these notes</a><br/>
    </p>
  </header>
</section>
<div style="display:none" id="mathjax_setup"> <!-- definitions for mathjax -->
  \[
  \newcommand{\pd}[2]{\frac{\partial #1}{\partial #2}}
  \newcommand{\bc}{{\bf c}}
  \newcommand{\bg}{{\bf g}}
  \newcommand{\bh}{{\bf h}}
  \newcommand{\bq}{{\bf q}}
  \newcommand{\bv}{{\bf v}}
  \newcommand{\bx}{{\bf x}}
  \newcommand{\by}{{\bf y}}
  \newcommand{\bu}{{\bf u}}
  \newcommand{\bw}{{\bf w}}
  \newcommand{\bz}{{\bf z}}
  \newcommand{\bA}{{\bf A}}
  \newcommand{\bB}{{\bf B}}
  \newcommand{\bC}{{\bf C}}
  \newcommand{\bH}{{\bf H}}
  \newcommand{\bI}{{\bf I}}
  \newcommand{\bJ}{{\bf J}}
  \newcommand{\bK}{{\bf K}}
  \newcommand{\bM}{{\bf M}}
  \newcommand{\bQ}{{\bf Q}}
  \newcommand{\bR}{{\bf R}}
  \newcommand{\bT}{{\bf T}}
  \newcommand{\balpha}{{\bf \alpha}}
  \newcommand{\bbeta}{{\bf \beta}}
  \newcommand{\blambda}{{\bf \lambda}}
  \newcommand{\btau}{{\bf \tau}}
  \newcommand{\bphi}{{\bf \phi}}
  \newcommand{\bPhi}{{\bf \Phi}}
  \newcommand{\bpsi}{{\bf \psi}}
  \newcommand{\bPsi}{{\bf \Psi}}
  \newcommand{\avg}[1]{E\left[ #1 \right]}
  \newcommand{\subjto}{\textrm{subject to}}
  \newcommand{\minimize}{\operatorname{\textrm{minimize}}}
  \newcommand{\maximize}{\operatorname{\textrm{maximize}}}
  \DeclareMathOperator*{\find}{find}
  \newcommand{\argmax}{\operatorname{\textrm{argmax}}}
  \newcommand{\argmin}{\operatorname{\textrm{argmin}}}
  \newcommand{\sgn}{\operatorname{\textrm{sgn}}}
  \newcommand{\trace}{\operatorname{\textrm{tr}}}
  \newcommand{\sos}{\text{ is SOS}}
  \]
</div>

<div id="debug_output"></div>

<p><b>Note:</b> These are working notes used for <a
href="http://manipulation.csail.mit.edu/">a course being taught at
MIT</a>. They will be updated throughout the Fall 2019 semester. </p>

<section id="table_of_contents"></section>


<section id="preface"><h1>Preface</h1>

  <p>I've always loved robots, but it's only relatively recently that I've
  turned my attention to robotic manipulation.  I particularly like the
  challenge of building robots that can master physics to achieve
  human/animal-like dexterity and agility.  It was
  <a
  href="http://underactuated.mit.edu/underactuated.html?chapter=intro">passive
  dynamic walkers</a> and the beautiful analysis that accompanies them that
  first helped cement this centrality of dynamics in my view of the world and my
  approach to robotics.  From there I became fascinated with (experimental)
  fluid dynamics, and the idea that birds with articulated wings actually
  "manipulate" the air to achieve incredible efficiency and agility. Humanoid
  robots and fast-flying aerial vehicles in clutter forced me to start thinking
  more deeply about the role of perception in dynamics and control.  Now I
  believe that this interplay between perception and dynamics is truly
  fundamental, and I am passionate about the observation that relatively
  "simple" problems in manipulation (how do I button up my dress shirt?) expose
  the problem beautifully.</p>

  <p>My approach to programming robots has always been very
  computational/algorithmic.  I started out using tools primarily from machine
  learning (especially reinforcement learning) to develop the control systems
  for simple walking machines; but as the robots and tasks got more complex I
  turned to more sophisticated tools from model-based planning and
  optimization-based control.  In my view, no other discipline has thought so
  deeply about dynamics as has control theory, and the algorithmic efficiency
  and guaranteed performance/robustness that can be obtained by the best
  model-based control algorithms far surpasses what we can do today with
  learning control.  Unfortunately, the mathematical maturity of
  controls-related research has also led the field to be relatively conservative
  in their assumptions and problem formulations; the requirements for robotic
  manipulation break these assumptions.  For example, robust control typically
  assumes that dynamics that are (nearly) smooth and uncertainty that can be
  represented by e.g. ellipsoidal or polytopic sets; but in robotic manipulation
  we must deal with the non-smooth mechanics of contact and uncertainty the
  comes from varied lighting conditions, and different numbers of objects with
  unknown geometry and dynamics.  In practice, no state-of-the-art robotic
  manipulation system to date (that I know of) uses rigorous control theory to
  design even the low-level feedback that determines when a robot makes and
  breaks contact with the objects it is manipulating.  An explicit goal of these
  notes is to try to change that.</p>

  <p>In the past few years, deep learning has had an unquestionable impact on robotic perception, unblocking some of the most daunting challenges in performing manipulation outside of a laboratory or factory environment.  We will discuss relevant tools from deep learning for object recognition, segmentation, pose/keypoint estimation, and even shape completion.  Now relatively old approaches to learning control are also enjoying an incredible surge in popularity, fueled in part by massive computing power and increasingly available robot hardware and simulators.  Unlike learning for perception, learning control algorithms are still far from a technology, with some of the most impressive looking results still being hard to understand and to reproduce.  But the recent work in this area has unquestionably highlighted the pitfalls of the conservatism taken by the controls community. Learning researchers are boldly formulating much more aggressive and exciting problems for robotic manipulation than we have seen before -- in many cases we are realizing that some manipulation tasks are actually quite easy, but in other cases we are finding problems that are still fundamentally hard.
  </p>

  <p>Finally, it feels that the time is ripe for robotic manipulation to have a real and dramatic impact in the world, in fields from logistics to home robots.  Over the last few years, we've seen UAVs/drones transition from academic curiosities into consumer products; autonomous cars have been transitioning from academic research to industry now, at least in terms of dollars invested.  Manipulation feels like the next big thing that will transition from robotic research to practice. It's still a bit risky for a venture capitalist to invest in, but nobody doubts the size of the market once we have the technology.  How lucky are we to potentially be able to play a role in that transition?</p>

  <p>So this is where the notes begin... we are at an incredible crossroads between learning and control and robotics with an opportunity to have immediate impact in industrial and consumer applications and potentially even to forge entirely new eras for systems theory and controls.  I'm just trying to hold on and to enjoy the ride.</p>

<!---
    <p>Data clearly has an immensely important role to play... (discussion slides from L4DC).  A pure learning approach is like sending our newborns out to play with the wolves.  Models and simulation just need to provide a sufficiently rich sand-box.  And it definitely seems short-sighted to bet against simulation.</p>
-->

</section> <!-- end preface -->

<section data-type="chapter" class="chapter" id="intro">
  <h1>Introduction</h1>

  <p>It's worth taking time to appreciate just how amazingly well humans are
  able to perform tasks with our hands.  Tasks that often feel mundane to us --
  loading the dishwasher, chopping vegetables, folding laundry -- remain as
  incredibly challenging tasks for robots and are at the very forefront of
  robotics research.</p>
  
  <todo>Add a diagram / photo of a the dish task here?</todo>

  <p>Consider the problem of picking up a single plate from a stack of plates in
  the sink and placing it into the dishwasher.  Clearly you first have to
  perceive that there is a plate in the sink and that this it is accessible.
  Getting your hand to the plate likely requires navigating your hand around the
  geometry of the sink and other dishes, and the act of actually picking it up
  might require a fairly subtle maneuver where you have to tip up the plate, sliding it along your fingers and along the sink/dishes in order to get a reasonable grasp on it.  Presumably as you lift it out of the sink, you'd like to mostly avoid collisions between the plate and the sink, which suggests a reasonable understanding of the size/extent of the plate (though note that robots tend to avoid collisions much more than humans do).  Even placing the plate into the dishwasher is pretty subtle; you might think that you would align the plate with the slats and then slide it in, but I think humans are more clever than that.  A seemingly better strategy is to loosen your grip on the plate, come in at an angle and intentionally contact one side of the slat... letting the plate effectively rotate itself into position as you set it down.  But the justification for this strategy is subtle -- it is a way to achieve the kinematically accurate task without requiring much kinematic accuracy on the position/orientation of the plate.</p>

  <todo>insert video of TRI dish pickup here</todo>

  <p>Perhaps one of the reasons that these problems remain so hard is that they
  require strong capabilities in numerous technical areas that have
  traditionally been somewhat disparate -- it's hard to be an expert in all of
  them.  More so than robotic mapping and navigation, or legged locomotion, or
  other great areas in robotics, the most interesting problems in manipulation
  require significant interactions between perception, planning, and control.
  This includes both geometric perception to understand the local geometry of
  the objects and environment, but also semantic perception to understand what
  opportunities for manipulation are avaiable in the scene. Planning typically
  includes both reasoning about the kinematic constraints of the task (how to I
  command my rigid seven degree-of-freedom arm to reach into the drawer?) as
  well as higher-level task planning (to get milk into my cup, I need to open
  the fridge, then grab the bottle, then unscrew the lid, then pour.. and
  perhaps even put it all back) that require understanding these kinematic
  constraints as well as the higher level task semantics.  And the lowest level,
  our hands are making and breaking contact with the world either directly or
  through tools, exerting forces, rapidly and easily transitioning between
  sticking and sliding frictional regimes -- these are incredible rich and
  difficult problems from the perspective of dynamics and control.</p>

  <todo>k-PAM?  Pete+Lucas imitation learning?</todo>

  <!-- two core research problems (that I like to focus on): 1) there are many tasks that we don't know how to program a robot to do robustly even in a single instance in lab (reach into your pocket and pull out the keys); 2) achieving robustness of a complete manipulation stack in the open-world. -->

  <section data-type="sect1"><h1>Manipulation is more than Pick-and-Place</h1></section>

    <p>There are a large number of applications for manipulation.  Picking up an
    object from one bin and placing it into another bin -- one version of the
    famous "pick and place" problem -- is a great application for robotic 
    manipulation.  And it is an application for which we already have pretty
    capable solutions, especially if the location/orientation of the placement
    need not be very accurate.  The recent advances in computer vision for
    object recognition make it very useful for a wide variety of industry
    applications: a camera can look into a bin and find an object of interest,
    then use a relatively simple strategy to do the pick.  This can be done with
    conventional robot hands or more special-purpose hands that are equipped
    with suction.  It can often be done without having a very accurate understanding of the shape, pose, mass, nor friction of the object(s) to be picked.</p>

    <p>The goal for these notes, however, is to examine the much broader view of
    manipulation that we is captured by the pick and place problem.  Even our thought experiment of loading the dishwasher -- arguably a more advanced type of pick and place -- requires much more from the perception, planning, and control systems.  But the diversity of tasks that humans (and hopefully soon robots) can do with their hands is truly remarkable.</p>

    <p>One field that has thought about the Motivating example: <a href="http://www.shap.ecs.soton.ac.uk/">South Hampton Assessment Procedure (SHAP)</a></p>

  <section data-type="sect1"><h1>Open-World Manipulation</h1>
  
    <p>Perhaps because humans are so good at manipulation, our expectations in terms of performance and robustness for these tasks is extremely high.  It's not enough to be able to load one set of place in a laboratory environment into a dishwasher reliably.  We'd like to be able to manipulate basically any plate that someone might put into the sink.  And we'd like the system to be able to work in any kitchen, despite various geometric configurations, lighting conditions, etc.  The challenge of achieving and verifying robustness of a complex manipulation stack with physics, perception, planning, and control in the loop is already daunting.  But how do we provide test coverage for every kitchen in the world?</p>

    <p>The idea that the world has infinite variability (there will never be a point at which you have seen every possible kitchen) is often referred to as the "open-world" or "open-domain" problem -- a term popularized first in the context of <a href="https://en.wikipedia.org/wiki/Open_world">video games</a>.</p>

    <p>Might it actually make the problem easier?  (Diversity changes the optimization landscape).</p>
  </section>

  <section data-type="sect1"><h1>Simulation</h1>
  
  </section>

  <section data-type="sect1">
    <h1>A Systems Theoretic Approach</h1>
  
    <blockquote>It's precisely <i>because</i> the problem is so complex that we take a rigorous approach.</blockquote>
  
    <p>We could draw this sort of diagram for many modern manipulation systems — they are often composed of numerous modules that communicate with eg IPC.  But i am going to demand a bit more of our models.  Explixitly declare their state, their parameters (just the ones that we would like to tune/analyze automatically), and any randomness.  This opens the door for more mature approaches to testing and verification, and ultimately for synthesis.  But it is surprisingly rare in robotics today. </p>
  
    <p>Probably don’t have to convince anyone about the value of the modularity.  Might have to convince about the value of declaring state, etc.  it will be a rexurring theme in this book.</p>
    
    <p>Using a drake system in your favorite ipc stack is easy.  We also make provisions for going the other way — taking your black-box component and using it in the systems framework.  But this limits which algorithms we can use on it.</p>
  
    <p>Idea: perception spoof.  Same input/output, mich simpler dynamics.  Ground truth + noise + dropouts.</p>
  
    <p>Runtime monitoring, system id, etc</p>
  
    <p>One thing that we’ll leave out is humans.  It’s not that i don’t like humans, but we have enough to do here without trying to model them.  Let’s call it future work.  The one exception is when we talk about control of the arm — a few simple heuristics at this level can make the robots much safer if operating around humans.  The robot won’t understand them, but we want to make sure we still don’t hurt them.  Or other robots/agents in the scene.</p>
  
  </section> <!-- end chapter (systems)-->

  <section data-type="sect1"><h1>Components of a Modern Manipulation System</h1>

    <p>The remaining chapters of these notes are organized...</p>
    <p>The central role of modeling and simulation.</p>
    <p>Robot Hardware.  Mathematical models.</p>

  </section>

</section> <!-- end chapter (intro)-->


  

<section data-type="chapter" class="chapter" id="hardware">
  <h1>Manipulation Hardware</h1>

  <todo>insert the manipulation station block diagram image here.</todo>
	
  <section data-type="sect1"><h1>Arms, Hands, Sensors, ...</h1></section>

  <section data-type="sect1"><h1>Low-cost arms</h1></section>

  <section data-type="sect1"><h1>Dexterous Hands</h1></section>

  <section data-type="sect1"><h1>Soft Hands/Skins</h1></section>

  <section data-type="sect1"><h1>Tactile Sensors</h1></section>

</section>

<section data-type="chapter" class="chapter" id="kinematics">
  <h1>Kinematics and Dynamics</h1>

</section>

<section data-type="chapter" class="chapter" id="pose">
  <h1>Geometric Perception I: Pose Estimation and Tracking</h1>

  <section data-type="sect1"><h1>Depth Sensors</h1>
  
    <p>Primarily RGB-D (ToF vs projected texture stereo vs ...) cameras and Lidar</p>

    <p>The cameras we are using in this course are <a href="https://software.intel.com/en-us/realsense/d400">Intel RealSense D415</a>....</p>

    <section data-type="sect2"><h1>Simulation</h1>
    
      <p>There are a number of levels of fidelity at which one can simulate a camera like the D415.  We'll start our discussion here using an "ideal" depth camera simulation -- the pixels returned represent the true geometric depth in the direction of each pixel coordinate.  In <drake></drake> that is represented by the <code>RgbdSensor</code> system, which can be wired up directly to the <code>SceneGraph</code>.</p>

      <table align="center" cellpadding="0" cellspacing="0">
        <tr align="center">
        <td><table cellspacing="0" cellpadding="0">
        <tr>
        <td align="right" style="padding:5px 0px 5px 0px">geometry_query &rarr;</td></tr>
        </table>
        </td><td align="center" style="border:solid;padding-left:20px;padding-right:20px" bgcolor="#F0F0F0"><a class="el" href="https://drake.mit.edu/doxygen_cxx/classdrake_1_1systems_1_1sensors_1_1_rgbd_sensor.html" title="A meta-sensor that houses RGB, depth, and label cameras, producing their corresponding images based o...">RgbdSensor</a></td><td><table cellspacing="0" cellpadding="0">
        <tr>
        <td align="left" style="padding:5px 0px 5px 0px">&rarr; color_image </td></tr>
        <tr>
        <td align="left" style="padding:5px 0px 5px 0px">&rarr; depth_image_32f </td></tr>
        <tr>
        <td align="left" style="padding:5px 0px 5px 0px">&rarr; depth_image_16u </td></tr>
        <tr>
        <td align="left" style="padding:5px 0px 5px 0px">&rarr; label_image </td></tr>
        <tr>
        <td align="left" style="padding:5px 0px 5px 0px">&rarr; X_WB </td></tr>
        </table>
        </td></tr>
        </table>

        <div data-type="example"><h1>Simulating an RGB-D Camera</h1>

          <p>As a simple example of depth cameras in drake, I've constructed a scene with a single object (the mustard bottle from the <a href="https://github.com/RobotLocomotion/models/tree/master/ycb">YCB dataset</a>), and added an <code>RgbdSensor</code> to the diagram.  Once this is wired up, we can simply evaluate the output ports in order to obtain the color and depth images:</p>
    
          <pysrc>depth_camera_demo/show_camera.py</pysrc>
    
          <p>Remember that, in addition to looking at the source code if you like, you can always inspect the block diagram to understand what is happen at the "systems level":</p>

          <pysrc>depth_camera_demo/show_diagram.py</pysrc>

          <p>(Note that these demos need to have the <code>demo.py</code> file that lives in the same directory to run).  <todo>Fix this!</todo></p>
        </div>
    
        <p>In the <code>ManipulationStation</code> simulation of the entire robot system for class, the <code>RgbdSensors</code> have already been added.  <a href="https://drake.mit.edu/doxygen_cxx/classdrake_1_1examples_1_1manipulation__station_1_1_manipulation_station.html">You can see image output ports for each of the cameras are already exported as output ports for the station.</a> </p>
        
        <p>Real depth sensors, of course, are far from ideal -- and errors in depth returns are not simple Gaussian noise, but rather are dependent on the lighting conditions, the surface normals of the object, and the visual material properties of the object, among other things.  We will examine real sensor data, and higher fidelity simulation models of the depth cameras, after we start understanding some of the basic operations.</p>

        <p>Please also note that, although the depth output of the RgbdSensor is ideal, this sensor also outputs a color image.  It's much harder to talk about an ideal color image, as colors change with material properties and lighting conditions (including reflections and shadows).  In the example above, I've told drake to just use the simplest OpenGL-based renderer.  But <drake></drake> does support a number of more advanced rendering options that we will leverage in the chapters on "deep perception", which make significantly more use of the RGB channels.</p>
    </section>
  </section> 

  <section data-type="sect1"><h1>Representations for Geometry</h1>
    
    <p>Depth image, point cloud, triangulated mesh, signed distance functions, (+ surfels, occupancy maps, ...)</p>

    <p>The data returned by a depth camera takes the form of an image, where each pixel value is a single number that represents the distance between the camera and the nearest object in the environment along the pixel direction.  If we combine this with the basic information about the camera's intrisic (e.g. lens parameters) and extrinsic (e.g. position and orientation in space) parameters of the camera, then we can transform this <i>depth image representation</i> into a collection of points in three dimensions, called the <i>point cloud representation</i>.  As depth sensors have become so pervasive the field has built up libraries of tools for performing basic goemetric operations operate on point clouds, and that can be used to transform back and forth between representations.  In these notes, we'll use the <a href="http://www.open3d.org/">Open3D</a> library (note that many older projects used the <a href="http://pointclouds.org/">Point Cloud Library (PCL)</a>, which is now defunct but still has some very useful documentation).</p>

    <div data-type="example"><h1>Getting RGB-D values into Open3D</h1>

      <p>Here the the mustard bottle example again, but this time we push the images coming out of <drake></drake> into the Open3D representation:</p>

      <pysrc>depth_camera_demo/show_open3d_rgbd.py</pysrc>
    </div>

    <p>It's important to realize that the conversion of a depth image into a point cloud does loose some information -- specifically the information about the ray that was cast from the camera to arrive at that point.  In addition to declaring "there is geometry at this point", the depth image also implies that "there is no geometry in the straight line path between camera and this point".  We will make use of this information in some of our algorithms, so don't discard the depth images completely!  More generally, we will find that each of the different geometry representations have strengths and weaknesses -- it is very common to keep multiple representations around and to convert back and forth between them.</p>

  </section>

  <section data-type="sect1"><h1>Working with Point Clouds</h1>

    <p>First, let us get a few simulated point clouds to work with.  We can use either <drake></drake> or Open3d to convert the depth image into a point cloud; we'll use Drake for this step here simply because we already have the camera parameters in the Drake format.  Although we can call the conversion functions directly, we can also add a system that does the conversion for us in the systems framework:</p>

    <table align="center" cellpadding="0" cellspacing="0">
      <tr align="center">
      <td><table cellspacing="0" cellpadding="0">
      <tr>
      <td align="right" style="padding:5px 0px 5px 0px">depth_image &rarr; </td></tr>
      <tr>
      <td align="right" style="padding:5px 0px 5px 0px">color_image (optional) &rarr; </td></tr>
      <tr>
      <td align="right" style="padding:5px 0px 5px 0px">camera_pose (optional) &rarr;</td></tr>
      </table>
      </td><td align="center" style="border:solid;padding-left:20px;padding-right:20px" bgcolor="#F0F0F0"><a class="el" href="http://drake.mit.edu/doxygen_cxx/classdrake_1_1perception_1_1_depth_image_to_point_cloud.html" title="Converts a depth image to a point cloud.">DepthImageToPointCloud</a></td><td><table cellspacing="0" cellpadding="0">
      <tr>
      <td align="left" style="padding:5px 0px 5px 0px">&rarr; point_cloud </td></tr>
      </table>
      </td></tr>
    </table>

    <p>You can infer from the optional input ports on this system that point cloud representations can potentially include color values for each of the Cartesian points.  The following example will get us a few point clouds to work with.</p>

    <div data-type="example"><h1>Getting a simulated point cloud</h1>

      <p>TODO: render the point cloud in either meshcat or open3d</p>

    </div>

    <todo>Combine all point clouds via https://drake.mit.edu/pydrake/pydrake.systems.perception.html?highlight=pointcloud#pydrake.systems.perception.PointCloudConcatenation</todo>

    <todo>Show the point cloud in Open3D's visualizer</todo>

    There are a number of basic operations that one can do on point clouds.  I'll give just a few examples here.  

    <section data-type="sect2"><h1>Surface Normal Estimation</h1>
    
      <p>The simplest and most straight-forward algorithm for estimating the normals of a point cloud surface is to find the $k$ nearest neighbors, and fit a plane through those points using least-squares.  Let's denote the nominal point (about which we are estimating the normal) $p_0$, and it's $k$ nearest neighboard $p_i, i\in[1,k].$  Let us form the $k \times 3$ data matrix: $$X = [ p_1 - p_0, p_2 - p_0, ..., p_k - p_0 ].$$  Then the principal components of this data matrix are given by the <a href="https://en.wikipedia.org/wiki/Singular_value_decomposition">singular value decomposition</a> of $X$ (or the eigenvectors of $X^T X$); if $X = U\Sigma V^T$, then the columns of $V$ are the principal directions, with the last column (associated with the smallest singular value) representing the normal direction.  The length can be normalized to one; but keep in mind that the sign of this vector is ambiguous -- the $k$ nearest points don't give any information about what direction might be into the object or out of the object.  This is a case where knowing the original camera direction can help -- for example, PCL offers a method <code>flipNormalTowardsViewpoint</code> that post-processes the normals.</p>

      <figure>
        <img src="http://www.pointclouds.org/assets/images/contents/documentation/features_normal.png" />

        <figcaption>Normal estimation using $k$ nearest neighbords (image linked from the <a href="http://docs.pointclouds.org/trunk/group__features.html"></a>PCL documentation</a>)</figcaption>
      </figure>

      <todo>Add an example; or even support normals in the DepthImageToPointCloud.  Example could be as simple as grabbing one point from the mustard point cloud, and finding it's normal (with everything plotted in matplotlib).</todo>

      <p>What was interesting and surprising (to me) about this is not that the least-squares solution works.  What is surprising is that this operation is considered commonplace and cheap -- for every point in the point cloud, we find $k$ nearest neighbors and do the normal estimation... even for very large point clouds.  Making this performant typically requires data structures like the <a href="https://en.wikipedia.org/wiki/K-d_tree">k-d tree</a> to make the nearest neighbor queries efficient.</p>

      <p>Another standard feature that can be extracted from the $k$ nearest neighbors is the local curvature.  The math is quite similar.  <todo>add it here</todo></p>

    </section>

    <section data-type="sect2"><h1>Plane Segmentation</h1>
    
      <p>Moving beyond local properties like surface normals and local curvature, we might choose to process a point cloud into primitive shapes -- like boxes or spheres.  Another practical and natural segmentation is segmenting surfaces into planes... which we can accomplish with heuristic algorithms which group neighboring points with similar surface normals.  We used this effectively in the DARPA Robotics Challenge.</p>

      <todo>Insert DRC plane segmentation videos here, from https://drive.google.com/drive/folders/1gYMJ0djBXbevWDBpekkK58pcbtZFr0A0 </todo>

    </section>

  </section>

  <section data-type="sect1"><h1>Geometric Pose Estimation</h1>

    <p>For many manipulation tasks, our goal is to estimate the pose of a particular known object in the scene.  In the extreme case, we might have a perfect mesh model of the object that we want to manipulation, and our task is to find the subset of the point cloud that can "fit" best to the known model.</p>

    <section data-type="sect2"><h1>Iterative Closest Point (ICP)</h1>

      <todo>Insert simple figure here, of model points and scene points.</todo>

      <p>One of the famous and often used (despite it's well-known limitations) is the interative closest point algorithm.  We'll start with the simplest version of it -- where instead of localizing a mesh model to the point cloud data, we will try localize two point clouds relative to each other. This is commonly referred to as "point-to-point ICP", "point cloud registration", or the "point correspondence problem".</p>

      <p>Let us denote the points on our model as $m_i \in M$, and the points obtained from our sensor as $s_i \in S$.  Even when we are assuming our point clouds are ideal (noise free), we will have a number of challenges: there is no reason to expect every point in $M$ to appear in $S$ (e.g., because our camera is viewing just one side of the object, or may be partially occluded), and there is no reason to believe that every element in $S$ corresponds to one of the points in $M$ (there may be other objects in the scene).  So a major task in ICP is to attempt to solve this <i>correspondence</i> problem.  But before we do, let us understand the case where the correspondences are magically given -- specifically we will assume that for every point of interest in the scene, $s_i$, we have a map $C$ such that $c_i$ is the index into the list of model points -- so $s_i$ corresponds with $m_{c_i}$.  Once these are given, our task is to recover the rigid body transformation that describes the relative pose of the model in the scene: \[ \min_{R \in SO(3), t \in R^3} \sum_i^N \| Rs_i + t - m_{c_i} \|^2, \] where $R$ is 3x3 rotation matrix and $t$ is a 3x1 translation vector.  At first glance, this looks like a nice optimization problem -- the decision variables $R$ and $t$ enter linearly in the inside term, so it is almost another simple least-squares problem. The trick is the constraint that $R$ must be a rotation matrix.  Therefore, we could equivalently have written \begin{align*} \min_{R \in R^{3x3}, t \in R^3} && \sum_i^N \| Rs_i + t - m_{c_i} \|^2, \\ \subjto && R^T = R^{-1}, \det(R) = 1. \end{align*} Surprisingly, if we ignore the determinant constraint, then this seeming difficult (quadratically constrainted quadratic program) has a closed form solution -- once again given (once again) by SVD.  Define the mean model and scene points $$\bar{m} = \frac{1}{N} \sum_i^N m_{c_i}, \qquad \bar{s} = \frac{1}{N} \sum_i^N s_i.$$  Compose the data matrix $$W = \sum_i (m_{c_i} - \bar{m}) (s_i - \bar{s})^T, $$ and use SVD to compute $W = U \Sigma V^T$.  Magically, the optimal solution is \begin{gather*} R^* = U V^T, \\ 
      t^* = \bar{m} - R \bar{s}.\end{gather*} There many derivations available in the literature, see <elib>Haralick89</elib> (section 3) for one of my favorites.  What is important for us to understand is that, once the correspondences are given, then we have an efficient closed-form solution to estimating the pose.</p>

      <todo>Point to plane ICP, Normal ICP</todo>
    
    </section>

    <todo>Add a simple example and a homework problem/exercise.</todo>


    <section data-type="sect2"><h1>Coherent Point Drift (CPD) and FilterReg</h1>

    </section>
  
    <section data-type="sect2"><h1>Global registration</h1>
  
      <todo>Greg's work.  Luca Carlone's work.  etc.</todo>

    </section>
  
  </section>

  <section data-type="sect1"><h1>Pose estimation as Inverse Kinematics</h1>
  
    <p>Dense articulated real-time tracking (DART).</p>

  </section>

  <section data-type="sect1"><h1>Tracking</h1>
  
  </section>

</section>

<section data-type="chapter" class="chapter" id="planning">
  <h1>Basic Planning and Control</h1>

  <section data-type="sect1"><h1>Task-space control</h1></section>

  <section
          data-type="sect1"><h1>Kinematic trajectory optimization</h1></section>

  <section data-type="sect1"><h1>Key-point formulations</h1></section>

</section>

<section data-type="chapter" class="chapter" id="grasping">
  <h1>Grasping</h1>

  <section data-type="sect1"><h1>Anti-podal grasps</h1></section>

  <section data-type="sect1"><h1>Grasp optimization</h1></section>

  <section data-type="sect1"><h1>Learning grasps</h1></section>

</section>

<section data-type="chapter" class="chapter" id="task">
    <h1>Programming the Task Level Execution</h1>

    <section data-type="sect1"><h1>Behavior Trees</h1></section>


</section>

<section data-type="chapter" class="chapter" id="fusion">
  <h1>Geometric Perception II: Dense Reconstructions</h1>

  <section data-type="sect1"><h1>Kinect Fusion</h1></section>

  <section data-type="sect1"><h1>Elastic Fusion</h1></section>

  <section data-type="sect1"><h1>SurfelWarp</h1></section>

</section>

<section data-type="chapter" class="chapter" id="planning">
  <h1>Planning in Cluttered Environments</h1>

</section>

<section data-type="chapter" class="chapter" id="perception">
  <h1>Deep Learning for Perception</h1>

  <section data-type="sect1"><h1>Getting training data</h1>

    <section data-type="sect2"><h1>LabelFusion</h1></section>

    <section data-type="sect2"><h1>Synthetic Datasets</h1></section>
  </section>

  <section data-type="sect1"><h1>Segmentation</h1></section>

  <section data-type="sect1"><h1>Object recognition</h1></section>

  <section data-type="sect1"><h1>Pose estimation</h1></section>

  <section data-type="sect1"><h1>Keypoint Detection</h1></section>

  <section data-type="sect1"><h1>Dense Descriptors</h1></section>

</section>

<section data-type="chapter" class="chapter" id="control">
  <h1>Feedback Control</h1>

</section>

<section data-type="chapter" class="chapter" id="planning">
  <h1>Non-Prehensile Manipulation</h1>

  <section data-type="sect1"><h1>Planning through Contact</h1></section>
  
</section>
  
<section data-type="chapter" class="chapter" id="belief">
  <h1>Planning under Uncertainty</h1>

</section>


<section data-type="chapter" class="chapter" id="tamp">
  <h1>Task and Motion Planning</h1>

</section>

<section data-type="chapter" class="chapter" id="rl">
  <h1>Reinforcement Learning</h1>

</section>

<section data-type="chapter" class="chapter" id="intuitive">
    <h1>Intuitive Physics</h1>

</section>

<section data-type="chapter" class="chapter" id="imitation">
  <h1>Teleoperation and Imitation Learning</h1>

  <!-- Not exactly sure where I want to put this, but it seems useful to have.  My current view is that this is an extremely useful set of tools for exploring the space of solutions, but probably not a final solution...  -->
  <p>Interfaces.  Inputs (keyboard + mouse, HTC Vive, data gloves...).  Feedback (VR, haptic feedback).</p>

</section>

<todo>Maybe a chapter on tactile feedback (estimation, tracking, slip detection, etc)?</todo>

<section data-type="chapter" class="chapter" id="v_and_v">
  <h1>Verification and Validation</h1>

  <p>P(world)</p>

</section>

<appendix class="part" title="Appendix">

  <section data-type="chapter" class="chapter" id="v_and_v">
    <h1>Robot Kinematics</h1>

    <p>Pose / Rigid Transform.  Rotation matrices, Euler angles, quaternions. </p>

    <p>Forward kinematics, inverse kinematics, Jacobians (analytic/geometric).</p>

    <!-- This is hard material to learn from text.  Have to do some simple exercises.  -->
      
  </section>
  

</appendix>


<div id="footer">
<hr>
<table style="width:100%;">
  <tr><td><em>Intelligent Robot Manipulation</em></td><td align="right">&copy;
    Russ
    Tedrake, 2019</td></tr>
</table>
</div>


</body>
</html>
