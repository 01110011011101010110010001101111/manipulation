<!DOCTYPE html>

<html>

  <head>
    <title>Robotic Manipulation: Geometric Pose Estimation</title>
    <meta name="Robotic Manipulation: Geometric Pose Estimation" content="text/html; charset=utf-8;" />
    <link rel="canonical" href="http://manipulation.csail.mit.edu/pose.html" />

    <script src="https://hypothes.is/embed.js" async></script>
    <script type="text/javascript" src="htmlbook/book.js"></script>

    <script src="htmlbook/mathjax-config.js" defer></script> 
    <script type="text/javascript" id="MathJax-script" defer
      src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js">
    </script>
    <script>window.MathJax || document.write('<script type="text/javascript" src="htmlbook/MathJax/es5/tex-chtml.js" defer><\/script>')</script>

    <link rel="stylesheet" href="htmlbook/highlight/styles/default.css">
    <script src="htmlbook/highlight/highlight.pack.js"></script> <!-- http://highlightjs.readthedocs.io/en/latest/css-classes-reference.html#language-names-and-aliases -->
    <script>hljs.initHighlightingOnLoad();</script>

    <link rel="stylesheet" type="text/css" href="htmlbook/book.css" />
  </head>

<body onload="loadChapter('manipulation');">

<div data-type="titlepage">
  <header>
    <h1><a href="index.html" style="text-decoration:none;">Robotic Manipulation</a></h1>
    <p data-type="subtitle">Perception, Planning, and Control</p> 
    <p style="font-size: 18px;"><a href="http://people.csail.mit.edu/russt/">Russ Tedrake</a></p>
    <p style="font-size: 14px; text-align: right;"> 
      &copy; Russ Tedrake, 2020<br/>
      Last modified <span id="last_modified"></span>.</br>
      <script>
      var d = new Date(document.lastModified);
      document.getElementById("last_modified").innerHTML = d.getFullYear() + "-" + (d.getMonth()+1) + "-" + d.getDate();</script>
      <a href="misc.html">How to cite these notes, use annotations, and give feedback.</a><br/>
    </p>
  </header>
</div>

<p><b>Note:</b> These are working notes used for <a
href="http://manipulation.csail.mit.edu/Fall2020/">a course being taught
at MIT</a>. They will be updated throughout the Fall 2020 semester.  <!-- <a 
href="https://www.youtube.com/channel/UChfUOAhz7ynELF-s_1LPpWg">Lecture  videos are available on YouTube</a>.--></p> 

<table style="width:100%;"><tr style="width:100%">
  <td style="width:33%;text-align:left;"><a class="previous_chapter" href=pick.html>Previous Chapter</a></td>
  <td style="width:33%;text-align:center;"><a href=index.html>Table of contents</a></td>
  <td style="width:33%;text-align:right;"><a class="next_chapter" href=deep_perception.html>Next Chapter</a></td>
</tr></table>


<!-- EVERYTHING ABOVE THIS LINE IS OVERWRITTEN BY THE INSTALL SCRIPT -->
<chapter style="counter-reset: chapter 3"><h1>Geometric Pose Estimation</h1>
  <a style="float:right; margin-top:-80px;" target="pose" href="https://colab.research.google.com/github/RussTedrake/manipulation/blob/master/pose.ipynb"><img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open Corresponding Notebook In Colab"/></a>
  <div style="clear:right;"></div>

  <p>In the last chapter, we developed an initial solution to moving objects
  around, but we made one major assumption that would prevent us from using it
  on a real robot: we assumed that we knew the initial pose of the object.  This
  chapter is going to be our first pass at removing that assumption, by
  developing tools to estimate that pose using the information obtained from the
  robot's depth cameras.  The tools we develop here will be most useful when you
  are trying to manipulate <i>known objects</i> (e.g. you have a mesh file of
  their geometry) and are in a relatively <i>uncluttered</i> environment.  But
  they will also form a baseline for the more sophisticated methods we will
  study.</p>

  <p>The approach that we'll take here is very geometric.  This is in contrast
  to, and very complimentary with, approached that are more fundamentally driven
  by data.  There is no question that deep learning has had an enormous impact
  in perception -- it's fair to say that it has enabling the current surge in
  manipulation advances -- and we will certainly cover it in these notes.  But
  when I've heard colleagues say that "all perception these days is based on
  deep learning", I can't help but cry foul.  There has been another surge of
  progress in the last few years that has been happening in parallel: the
  revolution in geometric reasoning, fueled by applications in autonomous
  driving and virtual/augmented reality in addition to manipulation.  Most
  advanced manipulation systems these days combine both "deep perception" and
  "geometric perception".</p>

  <section><h1>Cameras and depth sensors</h1>
  
    <p>Just as we had many choices when selecting the robot arm and hand, we
    have many choices for instrumenting our robot/environment with sensors. Even
    more so than our robot arms, the last few years have seen incredible
    improvements in the quality and reductions in cost and size for these
    sensors.  This is largely thanks to the cell phone industry, but the race
    for autonomous cars has been fueling high-end sensors as well.</p>

    <p>These changes in hardware quality have causes sometimes dramatic changes
    in our algorithmic approaches.  For example, estimation can be much easier
    when the resolution and frame rate of our sensors is high enough that not
    much can change in the world between two images; this undoubtedly
    contributed to the revolutions in the field of "simultaneous localization
    and mapping" (SLAM) we have seen over the last decade or so.</p>

    <p>One might think that the most important sensors for manipulation are the
    touch sensors (you might even be right!).  But in practice today, most of
    the emphasis is on camera-based and/or range sensing.  At very least, we
    should consider this first, since our touch sensors won't do us much good if
    we don't know where in the world we need to touch.</p>

    <p>Traditional cameras, which we think of as a sensor that outputs a color
    image at some framerate, play an important role.  But robotics makes heavy
    use of sensors that make an explicit measurement of the distance (between
    the camera and the world) or depth; sometimes in addition to color and
    sometimes in lieu of color.  Admittedly, some researchers think we should
    only rely on color images.</p>

    <subsection><h1>Depth sensors</h1>
    
      <p>Primarily RGB-D (ToF vs projected texture stereo vs ...) cameras and Lidar</p>

      <p>The cameras we are using in this course are <a href="https://software.intel.com/en-us/realsense/d400">Intel RealSense D415</a>....</p>

      <p>Monocular depth.</p>

      <todo>How the kinect works in 2 minutes: https://www.youtube.com/watch?v=uq9SEJxZiUg</todo>

    </subsection>

    <subsection><h1>Simulation</h1>
    
      <p>There are a number of levels of fidelity at which one can simulate a
      camera like the D415.  We'll start our discussion here using an "ideal"
      RGB-D camera simulation -- the pixels returned in the depth image
      represent the true geometric depth in the direction of each pixel
      coordinate.  In <drake></drake> that is represented by the
      <code>RgbdSensor</code> system, which can be wired up directly to the
      <code>SceneGraph</code>.</p>

      <div>
        <script src="htmlbook/js-yaml.min.js"></script>
        <script type="text/javascript">
        var sys = jsyaml.load(`
name: RgbdSensor
input_ports:
- geometry_query
output_ports:
- color_image
- depth_image_32f
- depth_image_16u
- label_image
- X_WB`);
        document.write(system_html(sys, "https://drake.mit.edu/doxygen_cxx/classdrake_1_1systems_1_1sensors_1_1_rgbd_sensor.html"));
        </script>
      </div>

      <p>The signals and systems abstraction here is encapsulating a lot of
      complexity.  Inside the implementation of that system is a complete
      rendering engine, like one that you will find in high-end computer games.
      Drake actually support multiple rendering engines; for the purposes of
      this class we will primarily use an OpenGL-based renderer that is suitable
      for real-time simulation.  In our research we also use Drake's rendering
      API to connect to high-end <a
      href="https://en.wikipedia.org/wiki/Physically_based_rendering">physically-based
      rendering (PBR)</a> based on ray tracing, such as the <a
      href="https://www.blender.org/features/rendering/">Cycles renderer
      provided by Blender</a>.  These are most useful when we are trying to
      render higher quality images e.g. to <i>train</i> a deep perception
      system; we will discuss them in the deep perception chapters.</p>

      <example class="drake"><h1>Simulating an RGB-D camera</h1>

        <p><a target="pose"
          href="https://colab.research.google.com/github/RussTedrake/manipulation/blob/master/pose.ipynb#scrollTo=7q0A14bAilIX"><img
          src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"/></a>
          </p>

        <p>As a simple example of depth cameras in drake, I've constructed a
        scene with a single object (the mustard bottle from the <a
        href="https://github.com/RobotLocomotion/models/tree/master/ycb">YCB
        dataset</a>), and added an <code>RgbdSensor</code> to the diagram.  Once
        this is wired up, we can simply evaluate the output ports in order to
        obtain the color and depth images:</p>

        <figure><img src="data/mustard.svg"/></figure>
    
        <p>Remember that, in addition to looking at the source code if you like,
        you can always inspect the block diagram to understand what is happening
        at the "systems level".  <a href="data/depth_camera_diagram.svg">Here is the diagram</a> used in this example.</p>

      </example>
    
      <p>In the <code>ManipulationStation</code> simulation of the entire robot
      system for class, the <code>RgbdSensors</code> have already been added,
      and their output ports exposed as outputs for the station.</p>
        
      <p>Real depth sensors, of course, are far from ideal -- and errors in
      depth returns are not simple Gaussian noise, but rather are dependent on
      the lighting conditions, the surface normals of the object, and the visual
      material properties of the object, among other things.  The color channels
      are an approximation, too.  Even our high-end renderers can only do as
      well as the geometries, materials and lighting sources that we add to our
      scene, and it is very hard to capture all of the nuances of a real
      environment.  We will examine real sensor data, and the gaps between
      modeled sensors and real sensors, after we start understanding some of the
      basic operations.</p>

      <figure><img src="data/Sweeney18a_fig3.png"/><figcaption>Real depth
      sensors are messy.  The red pixels in the far right image are missed
      returns -- the depth sensor returned "maximum depth".  Missed returns,
      especially on the edges of objects (or on reflective objects) are a common
      sighting in raw depth data.  This figure is reproduced from
      <elib>Sweeney18a</elib>.</figcaption></figure>
    </subsection>
  </section>

  <section><h1>Representations for geometry</h1>

    <p>Depth image, point cloud, triangulated mesh, signed distance functions, (+ surfels, occupancy maps, ...)</p>

    <p>The data returned by a depth camera takes the form of an image, where
    each pixel value is a single number that represents the distance between the
    camera and the nearest object in the environment along the pixel direction.
    If we combine this with the basic information about the camera's intrinsic
    parameters (e.g. lens parameters, stored in the <a
    href="https://drake.mit.edu/doxygen_cxx/classdrake_1_1systems_1_1sensors_1_1_camera_info.html"><code>CameraInfo</code></a>
    class in Drake) then we can transform this <i>depth image representation</i>
    into a collection of 3D points, $s_i$.  I use $s$ here because they are
    commonly referred to as the "scene points" in the algorithms we will present
    below.  These points have a pose and (optionally) a color value or other
    information attached.  This is known as a <i>point cloud representation</i>.
    By default, their pose is described in the camera frame, $^CX^{s_i}$, but
    the <code>DepthImageToPointCloud</code> system in Drake also accepts $^PX^C$
    to make it easy to transform them into another frame (such as the world
    frame).</p>  
    
    <div>
      <script src="htmlbook/js-yaml.min.js"></script>
      <script type="text/javascript">
      var sys = jsyaml.load(`
name: DepthImageToPointCloud
input_ports:
- depth_image
- color_image (optional)
- camera_pose (optional)
output_ports:
- point_cloud`);
      document.write(system_html(sys, "http://drake.mit.edu/doxygen_cxx/classdrake_1_1perception_1_1_depth_image_to_point_cloud.html"));
      </script>
    </div>
    
    <p>As depth sensors have become so pervasive the field has built up
    libraries of tools for performing basic goemetric operations operate on
    point clouds, and that can be used to transform back and forth between
    representations.  We can implement many of of the basic operations directly
    in Drake, but I would recommend the <a
    href="http://www.open3d.org/">Open3D</a> library if you need more (I have an
    example of transferring data from the Drake image and point cloud
    representations into Open3D <a href="manipulation/open3d/">here</a>).  Many
    older projects used the <a href="http://pointclouds.org/">Point Cloud
    Library (PCL)</a>, which is now defunct but still has some very useful
    documentation.</p>

    <p>It's important to realize that the conversion of a depth image into a
    point cloud does loose some information -- specifically the information
    about the ray that was cast from the camera to arrive at that point.  In
    addition to declaring "there is geometry at this point", the depth image
    combined with the camera pose also implies that "there is no geometry in the
    straight line path between camera and this point".  We will make use of this
    information in some of our algorithms, so don't discard the depth images
    completely!  More generally, we will find that each of the different
    geometry representations have strengths and weaknesses -- it is very common
    to keep multiple representations around and to convert back and forth
    between them.</p>

  </section>

  <!--
  <section><h1>Working with point clouds</h1>

    <p>First, let us get a few simulated point clouds to work with.  We can use either <drake></drake> or Open3d to convert the depth image into a point cloud; we'll use Drake for this step here simply because we already have the camera parameters in the Drake format.  Although we can call the conversion functions directly, we can also add a system that does the conversion for us in the systems framework:</p>

    <table align="center" cellpadding="0" cellspacing="0">
      <tr align="center">
      <td><table cellspacing="0" cellpadding="0">
      <tr>
      <td align="right" style="padding:5px 0px 5px 0px">depth_image &rarr; </td></tr>
      <tr>
      <td align="right" style="padding:5px 0px 5px 0px">color_image (optional) &rarr; </td></tr>
      <tr>
      <td align="right" style="padding:5px 0px 5px 0px">camera_pose (optional) &rarr;</td></tr>
      </table>
      </td><td align="center" style="border:solid;padding-left:20px;padding-right:20px" bgcolor="#F0F0F0"><a class="el" href="http://drake.mit.edu/doxygen_cxx/classdrake_1_1perception_1_1_depth_image_to_point_cloud.html" title="Converts a depth image to a point cloud.">DepthImageToPointCloud</a></td><td><table cellspacing="0" cellpadding="0">
      <tr>
      <td align="left" style="padding:5px 0px 5px 0px">&rarr; point_cloud </td></tr>
      </table>
      </td></tr>
    </table>

    <p>You can infer from the optional input ports on this system that point cloud representations can potentially include color values for each of the Cartesian points.  The following example will get us a few point clouds to work with.</p>

    <todo>Combine all point clouds via https://drake.mit.edu/pydrake/pydrake.systems.perception.html?highlight=pointcloud#pydrake.systems.perception.PointCloudConcatenation</todo>

    <todo>Show the point cloud in Open3D's visualizer</todo>

    There are a number of basic operations that one can do on point clouds.  I'll give just a few examples here.  

    <subsection><h1>Surface normal estimation</h1>
    
      <p>The simplest and most straight-forward algorithm for estimating the normals of a point cloud surface is to find the $k$ nearest neighbors, and fit a plane through those points using least-squares.  Let's denote the nominal point (about which we are estimating the normal) $p_0$, and it's $k$ nearest neighboard $p_i, i\in[1,k].$  Let us form the $k \times 3$ data matrix: $$X = [ p_1 - p_0, p_2 - p_0, ..., p_k - p_0 ].$$  Then the principal components of this data matrix are given by the <a href="https://en.wikipedia.org/wiki/Singular_value_decomposition">singular value decomposition</a> of $X$ (or the eigenvectors of $X^T X$); if $X = U\Sigma V^T$, then the columns of $V$ are the principal directions, with the last column (associated with the smallest singular value) representing the normal direction.  The length can be normalized to one; but keep in mind that the sign of this vector is ambiguous -- the $k$ nearest points don't give any information about what direction might be into the object or out of the object.  This is a case where knowing the original camera direction can help -- for example, PCL offers a method <code>flipNormalTowardsViewpoint</code> that post-processes the normals.</p>

      <figure>
        <img src="http://www.pointclouds.org/assets/images/contents/documentation/features_normal.png" />

        <figcaption>Normal estimation using $k$ nearest neighbords (image linked from the <a href="http://docs.pointclouds.org/trunk/group__features.html"></a>PCL documentation</a>)</figcaption>
      </figure>

      <todo>Add an example; or even support normals in the DepthImageToPointCloud.  Example could be as simple as grabbing one point from the mustard point cloud, and finding it's normal (with everything plotted in matplotlib).</todo>

      <p>What was interesting and surprising (to me) about this is not that the least-squares solution works.  What is surprising is that this operation is considered commonplace and cheap -- for every point in the point cloud, we find $k$ nearest neighbors and do the normal estimation... even for very large point clouds.  Making this performant typically requires data structures like the <a href="https://en.wikipedia.org/wiki/K-d_tree">k-d tree</a> to make the nearest neighbor queries efficient.</p>

      <p>Another standard feature that can be extracted from the $k$ nearest neighbors is the local curvature.  The math is quite similar.  <todo>add it here</todo></p>

    </subsection>

    <subsection><h1>Plane segmentation</h1>
    
      <p>Moving beyond local properties like surface normals and local curvature, we might choose to process a point cloud into primitive shapes -- like boxes or spheres.  Another practical and natural segmentation is segmenting surfaces into planes... which we can accomplish with heuristic algorithms which group neighboring points with similar surface normals.  We used this effectively in the DARPA Robotics Challenge.</p>

      <todo>Insert DRC plane segmentation videos here, from https://drive.google.com/drive/folders/1gYMJ0djBXbevWDBpekkK58pcbtZFr0A0 </todo>

    </subsection>

  </section>
  -->

  <section><h1>Geometric pose estimation</h1>

    <p>For many manipulation tasks, our goal is to estimate the pose of a particular known object in the scene.  In the extreme case, we might have a perfect mesh model of the object that we want to manipulation, and our task is to find the subset of the point cloud that can "fit" best to the known model.</p>

    <subsection><h1>Iterative Closest Point (ICP)</h1>

      <p>One of the famous and often used (despite it's well-known limitations) is the interative closest point algorithm.  We'll start with the simplest version of it -- where instead of localizing a mesh model to the point cloud data, we will try localize two point clouds relative to each other. This is commonly referred to as "point-to-point ICP", "point cloud registration", or the "point correspondence problem".</p>

      <figure>
        <img src="figures/ICP_cartoon.jpg" width="80%"/>
        <figcaption>Model points and scene points.</figcaption>
      </figure>

      <p>Let us denote the points on our model as $m_i \in M$, and the points obtained from our sensor as $s_i \in S$.  Even when we are assuming our point clouds are ideal (noise free), we will have a number of challenges: there is no reason to expect every point in $M$ to appear in $S$ (e.g., because our camera is viewing just one side of the object, or may be partially occluded), and there is no reason to believe that every element in $S$ corresponds to one of the points in $M$ (there may be other objects in the scene).  So a major task in ICP is to attempt to solve this <i>correspondence</i> problem.  But before we do, let us understand the case where the correspondences are magically given -- specifically we will assume that for every point of interest in the scene, $s_i$, we have a map $C$ such that $c_i$ is the index into the list of model points -- so $s_i$ corresponds with $m_{c_i}$.  Once these are given, our task is to recover the rigid body transformation that describes the relative pose of the model in the scene: \[ \min_{R \in SO(3), t \in \mathbb{R}^3} \sum_i^N \| Rs_i + t - m_{c_i} \|^2, \] where $R$ is 3x3 rotation matrix and $t$ is a 3x1 translation vector.  At first glance, this looks like a nice optimization problem -- the decision variables $R$ and $t$ enter linearly in the inside term, so it is almost another simple least-squares problem. The trick is the constraint that $R$ must be a rotation matrix.  Therefore, we could equivalently have written \begin{align*} \min_{R \in \mathbb{R}^{3x3}, t \in \mathbb{R}^3} && \sum_i^N \| Rs_i + t - m_{c_i} \|^2, \\ \subjto && R^T = R^{-1}, \det(R) = 1. \end{align*} Surprisingly, if we ignore the determinant constraint, then this seeming difficult (quadratically constrainted quadratic program) has a clean numerical solution -- once again given (once again) by SVD.  Define the mean model and scene points $$\bar{m} = \frac{1}{N} \sum_i^N m_{c_i}, \qquad \bar{s} = \frac{1}{N} \sum_i^N s_i.$$  Compose the data matrix $$W = \sum_i (m_{c_i} - \bar{m}) (s_i - \bar{s})^T, $$ and use SVD to compute $W = U \Sigma V^T$.  Magically, the optimal solution is \begin{gather*} R^* = U V^T, \\ 
      t^* = \bar{m} - R \bar{s}.\end{gather*} Sometimes this formulation of gives a Householder reflection matrix instead of a rotation matrix.  If this is the case, which can be checked if the determinant of $R^*$ is -1, you must negate the third column of $V$, then recompute $R^*$.
      There many derivations available in the literature, see <elib>Haralick89</elib> (section 3) for one of my favorites.  What is important for us to understand is that, once the correspondences are given, then we have an efficient and robust numerical solution to estimating the pose.</p>

      <p>Let us now consider the other half of the problem.  What if the pose, given by $R$ and $t$, was known... then can we back out the optimal correspondences?  If we aim to maximize our reconstruction objective, then the optimal correspondences are given by $$c_i = \argmin_{j \in N_m}\| R s_i + t - m_{c_j} \|^2,$$  where $N_m$ is the number of model points.  In words, we want to find the point in the model that is the closest in Euclidean distance to the transformed scene points.  This is the famous "nearest neighbor" problem, and we have good numerical solutions (often using the aforementioned special data structures) for it.</p>

      <p>Although solving for the pose and the correspondences jointly is very difficult, ICP leverages the idea if we solve for them independently, then both parts have good solutions.  So the iterative algorithm simply starts with an initial guess of $R$ and $t$, estimates the correspondences by finding the nearest neighbords, then computes a new $R$ and $t$, and so on until this iteration converges.  I like to think of this as a specific instance of the general solution approaches taken in optimization for e.g. bilinear optimization or expectation maximization.</p>

      <p>It is important to understand that this is a local solution to a non-convex optimization problem.  So it is subject to getting stuck in local minima.  Thanks to the geometric nature of the problem, many of these local minima are graphical and intuitive.  (Will insert a few examples here). </p>

      <p>Intuition about these local minima has motivated a number of ICP variants, include point to plane ICP, normal ICP, ICP that use color information, feature-based ICP, etc.</p>

      <p>Maybe more fundamental, though, is the fact that I don't think the ICP objective, even if we could optimize it perfectly is quite right.  (TODO: add the example of a long thin book).  And there is information that we have, which should aid in estimating the pose, which is not present in the point cloud (like the location of the cameras).</p>
    
    </subsection>

    <todo>Add a simple example and a homework problem/exercise.</todo>


    <subsection><h1>Pose estimation as inverse kinematics</h1>
  
      <p>Dense articulated real-time tracking (DART).</p>
  
    </subsection>
  
    <subsection><h1>Coherent Point Drift (CPD) and FilterReg</h1>

    </subsection>
  
    <subsection><h1>Global registration</h1>
  
      <todo>Greg's work.  Luca Carlone's work.  etc.</todo>

    </subsection>
  
  </section>


  <section><h1>Tracking</h1>
  
  </section>

</chapter>
<!-- EVERYTHING BELOW THIS LINE IS OVERWRITTEN BY THE INSTALL SCRIPT -->

<table style="width:100%;"><tr style="width:100%">
  <td style="width:33%;text-align:left;"><a class="previous_chapter" href=pick.html>Previous Chapter</a></td>
  <td style="width:33%;text-align:center;"><a href=index.html>Table of contents</a></td>
  <td style="width:33%;text-align:right;"><a class="next_chapter" href=deep_perception.html>Next Chapter</a></td>
</tr></table>

<div id="footer">
  <hr>
  <table style="width:100%;">
    <tr><td><em>Robotic Manipulation</em></td><td style="text-align:right">&copy; Russ
      Tedrake, 2020</td></tr>
  </table>
</div>


</body>
</html>
