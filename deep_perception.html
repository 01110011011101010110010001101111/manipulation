<!DOCTYPE html>

<html>

  <head>
    <title>Robotic Manipulation: Data-driven perception: object
detection, segmentation, and pose estimation</title>
    <meta name="Robotic Manipulation: Data-driven perception: object
detection, segmentation, and pose estimation" content="text/html; charset=utf-8;" />
    <link rel="canonical" href="http://manipulation.csail.mit.edu/deep_perception.html" />

    <script src="https://hypothes.is/embed.js" async></script>
    <script type="text/javascript" src="htmlbook/book.js"></script>

    <script src="htmlbook/mathjax-config.js" defer></script> 
    <script type="text/javascript" id="MathJax-script" defer
      src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js">
    </script>
    <script>window.MathJax || document.write('<script type="text/javascript" src="htmlbook/MathJax/es5/tex-chtml.js" defer><\/script>')</script>

    <link rel="stylesheet" href="htmlbook/highlight/styles/default.css">
    <script src="htmlbook/highlight/highlight.pack.js"></script> <!-- http://highlightjs.readthedocs.io/en/latest/css-classes-reference.html#language-names-and-aliases -->
    <script>hljs.initHighlightingOnLoad();</script>

    <link rel="stylesheet" type="text/css" href="htmlbook/book.css" />
  </head>

<body onload="loadChapter('manipulation');">

<div data-type="titlepage">
  <header>
    <h1><a href="index.html" style="text-decoration:none;">Robotic Manipulation</a></h1>
    <p data-type="subtitle">Perception, Planning, and Control</p> 
    <p style="font-size: 18px;"><a href="http://people.csail.mit.edu/russt/">Russ Tedrake</a></p>
    <p style="font-size: 14px; text-align: right;"> 
      &copy; Russ Tedrake, 2020<br/>
      Last modified <span id="last_modified"></span>.</br>
      <script>
      var d = new Date(document.lastModified);
      document.getElementById("last_modified").innerHTML = d.getFullYear() + "-" + (d.getMonth()+1) + "-" + d.getDate();</script>
      <a href="misc.html">How to cite these notes, use annotations, and give feedback.</a><br/>
    </p>
  </header>
</div>

<p><b>Note:</b> These are working notes used for <a
href="http://manipulation.csail.mit.edu/Fall2020/">a course being taught
at MIT</a>. They will be updated throughout the Fall 2020 semester.  <!-- <a 
href="https://www.youtube.com/channel/UChfUOAhz7ynELF-s_1LPpWg">Lecture  videos are available on YouTube</a>.--></p> 

<table style="width:100%;"><tr style="width:100%">
  <td style="width:33%;text-align:left;"><a class="previous_chapter" href=clutter.html>Previous Chapter</a></td>
  <td style="width:33%;text-align:center;"><a href=index.html>Table of contents</a></td>
  <td style="width:33%;text-align:right;"><a class="next_chapter" href=grasping.html>Next Chapter</a></td>
</tr></table>


<!-- EVERYTHING ABOVE THIS LINE IS OVERWRITTEN BY THE INSTALL SCRIPT -->
<chapter style="counter-reset: chapter 5"><h1>Data-driven perception: object
detection, segmentation, and pose estimation</h1>

  <p>Our study of geometric perception gave us good tools for estimating the
  pose of a known object.  These algorithms can produce highly accurate
  estimates, but are still subject to local minima.  When the scenes get more
  cluttered/complicated, or if we are dealing with many different object types,
  they really don't offer an adequate solution by themselves.</p>

  <p>Deep learning has given us data-driven solutions that complement our
  geometric approaches beautifully.  Finding correlations in massive datasets
  has proven to be a fantastic way to provide practical solutions to these more
  "global" problems like detecting whether the mustard bottle is even in the
  scene, segmenting out the portion of the image / point cloud that is relevant
  to the object, and even in providing a rough estimate of the pose that could
  be refined with a geometric method.</p>

  <p>There are many sources of information about deep learning on the internet,
  and I have no ambition of replicating nor replacing them here.  But I do hope
  to give you a sense for the problem formulations that have worked well for
  deep perception in manipulation, and the sources of data that can potentially
  provide the relevant training and test sets.</p>

  <section><h1>Getting to big data</h1>

    <subsection><h1>Crowd-sourced annotation datasets</h1>
    
      <p>The modern revolution in computer vision was unquestionably fueled by
      the availability of massive annotated datasets.  The most famous of all is
      ImageNet, which eclipsed previous datasets with the number of images and
      the accuracy and usefulness of the labels<elib>Russakovsky15</elib>.
      Fei-fei Li, who led the creation of ImageNet has been giving talks that
      give some nice historical perspective on how ImageNet came to be.  
      <a href="https://youtu.be/0bb9EcaQupI">Here is one</a> (slightly) tailored
      to robotics and even manipulation; you might start <a
      href="https://youtu.be/0bb9EcaQupI?t=857">here</a>.  </p>

      <p><elib>Russakovsky15</elib> describes the annotations available in
      ImageNet: <blockquote>... annotations fall into one of two categories: (1)
      image-level annotation of a binary label for the presence or absence of an
      object class in the image, e.g., "there are cars in this image" but "there
      are no tigers," and (2) object-level annotation of a tight bounding box
      and class label around an object instance in the image, e.g., "there is a
      screwdriver centered at position (20,25) with width of 50 pixels and
      height of 30 pixels".</blockquote></p>

      <figure><img width="80%"
      src="data/coco_instance_segmentation.jpeg"/><figcaption>A sample annotated
      image from the COCO dataset, illustrating the difference between
      image-level annotations, object-level annotations, and segmentations at
      the class/semantic- or instance- level..</figcaption></figure>

      <p>The COCO dataset similarly enabled pixel-wise <i>instance-level</i>
      segmentation <elib>Lin14a</elib>.  It has fewer object categories than
      ImageNet, but more instances per category. It's still shocking to me that
      they were able to get 2.5 million images labeled at the pixel level.  Back
      when they were working on LabelMe <elib>Russell08</elib>, Antonio Torralba
      used to joke about how surprised he was about the accuracy of the (nearly)
      pixel-wise annotations that he was able to crowd-source (and that his
      mother was a particularly prolific and accurate labeler)!</p>

      <p>These existing datasets are often general enough, and contain enough
      useful categories that perception systems trained on this data can
      sometimes be used right "out of the box".  But to improve performance, it
      almost always makes sense to generate more labeled data for the objects /
      scenes that we actually care about.  Remarkably, even when we do collect
      more specialized data for any particular application, it is still common
      practice to start with a network trained on these more general datasets,
      and then "fine-tune" it to our application.  In fact, the last few years
      have seen a number of start-ups based purely on the idea that they would
      help streamline the process of getting your dataset labeled for you.  But
      thankfully, this isn't our only option.
      </p>
    
    </subsection>

    <subsection><h1>Annotation tools for manipulation</h1></subsection>

    <subsection><h1>Synthetic datasets</h1></subsection>


  </section>

  <section><h1>Object detection and segmentation</h1>
  
    <!--Get to bounding boxes.  yolo, etc.-->

    <!-- nice overview slides at http://cs231n.stanford.edu/slides/2020/lecture_12.pdf .  Videos at https://www.youtube.com/watch?v=vT1JzLTH4G4&list=PL3FW7Lu3i5JvHM8ljYj-zLfQRF3EO8sYv&index=1 -->
  
    <p>Fully Convolutional Networks (FCN) for semantic segmentation
    <elib>Long15</elib>.  Girschink et al introduced "Regions with CNN Features"
    (R-CNN), and then improvements to make it "Fast" and "Faster"
    <elib>Girshick14+Girshick15+Ren15</elib>.  Mask R-CNN <elib>He17</elib>.  We will use the
    latest implementation in <a
    href="https://github.com/facebookresearch/detectron2">Detectron2</a> from
    Facebook AI Research.</p>

  </section>

  <section><h1>Pose estimation</h1>
  
    <p>Representations of SE(3).</p>

    <p>Area under the curve plots, etc.</p>
  
  </section>

  <!--
  <section><h1>Keypoint detection</h1></section>

  <section><h1>Dense Descriptors</h1></section>
-->    
</chapter>
<!-- EVERYTHING BELOW THIS LINE IS OVERWRITTEN BY THE INSTALL SCRIPT -->

<table style="width:100%;"><tr style="width:100%">
  <td style="width:33%;text-align:left;"><a class="previous_chapter" href=clutter.html>Previous Chapter</a></td>
  <td style="width:33%;text-align:center;"><a href=index.html>Table of contents</a></td>
  <td style="width:33%;text-align:right;"><a class="next_chapter" href=grasping.html>Next Chapter</a></td>
</tr></table>

<div id="footer">
  <hr>
  <table style="width:100%;">
    <tr><td><em>Robotic Manipulation</em></td><td style="text-align:right">&copy; Russ
      Tedrake, 2020</td></tr>
  </table>
</div>


</body>
</html>
